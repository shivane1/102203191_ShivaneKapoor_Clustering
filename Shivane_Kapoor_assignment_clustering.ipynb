{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "z3OhnWsCyHHP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, MeanShift\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris = datasets.load_iris()\n",
        "X = iris.data"
      ],
      "metadata": {
        "id": "OqqWqNX3yWXP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def no_processing(X): return X\n",
        "def normalization(X): return MinMaxScaler().fit_transform(X)\n",
        "def transform(X): return StandardScaler().fit_transform(X)\n",
        "def pca(X): return PCA(n_components=2).fit_transform(X)\n",
        "def t_n(X): return StandardScaler().fit_transform(MinMaxScaler().fit_transform(X))\n",
        "def t_n_pca(X): return PCA(n_components=2).fit_transform(StandardScaler().fit_transform(MinMaxScaler().fit_transform(X)))\n",
        "\n",
        "preprocessing_methods = {\n",
        "    \"No Data Processing\": no_processing,\n",
        "    \"Using Normalization\": normalization,\n",
        "    \"Using Transform\": transform,\n",
        "    \"Using PCA\": pca,\n",
        "    \"Using T+N\": t_n,\n",
        "    \"T+N+PCA\": t_n_pca\n",
        "}"
      ],
      "metadata": {
        "id": "VZC5CofyyWZ6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cluster_nums = [2, 4, 5]\n",
        "results = {\n",
        "    \"KMeans\": {},\n",
        "    \"Agglomerative\": {},\n",
        "    \"MeanShift\": {}\n",
        "}"
      ],
      "metadata": {
        "id": "8u0Ai2KayWiQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_clustering(model, data):\n",
        "    try:\n",
        "        labels = model.fit_predict(data)\n",
        "        sil = silhouette_score(data, labels)\n",
        "        cal = calinski_harabasz_score(data, labels)\n",
        "        dav = davies_bouldin_score(data, labels)\n",
        "        return sil, cal, dav\n",
        "    except Exception:\n",
        "        return \"NA\", \"NA\", \"NA\"\n",
        "\n",
        "def process_meanshift(data, expected_clusters=3):\n",
        "    try:\n",
        "        ms_model = MeanShift()\n",
        "        labels = ms_model.fit_predict(data)\n",
        "        if len(np.unique(labels)) == expected_clusters:\n",
        "            sil = silhouette_score(data, labels)\n",
        "            cal = calinski_harabasz_score(data, labels)\n",
        "            dav = davies_bouldin_score(data, labels)\n",
        "            return sil, cal, dav\n",
        "    except Exception:\n",
        "        pass\n",
        "    return \"NA\", \"NA\", \"NA\"\n",
        "\n",
        "for method_name, method_func in preprocessing_methods.items():\n",
        "    processed_data = method_func(X)\n",
        "\n",
        "    for num_clusters in cluster_nums:\n",
        "        kmeans_model = KMeans(n_clusters=num_clusters, n_init=10, random_state=42)\n",
        "        kmeans_scores = evaluate_clustering(kmeans_model, processed_data)\n",
        "        results[\"KMeans\"].setdefault(method_name, []).append(kmeans_scores)\n",
        "\n",
        "        agg_model = AgglomerativeClustering(n_clusters=num_clusters)\n",
        "        agg_scores = evaluate_clustering(agg_model, processed_data)\n",
        "        results[\"Agglomerative\"].setdefault(method_name, []).append(agg_scores)\n",
        "\n",
        "        if num_clusters == 3:\n",
        "            ms_scores = process_meanshift(processed_data)\n",
        "        else:\n",
        "            ms_scores = (\"NA\", \"NA\", \"NA\")\n",
        "        results[\"MeanShift\"].setdefault(method_name, []).append(ms_scores)\n"
      ],
      "metadata": {
        "id": "mhQTJe0PyWk-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_results_table(algorithm, preprocessing_methods, clusters, result_data):\n",
        "    metrics = [\"Silhouette\", \"Calinski-Harabasz\", \"Davies-Bouldin\"]\n",
        "    data = []\n",
        "\n",
        "    for prep in preprocessing_methods:\n",
        "        for i, c in enumerate(clusters):\n",
        "            row = {\n",
        "                \"Preprocessing\": prep,\n",
        "                \"Clusters\": c\n",
        "            }\n",
        "            try:\n",
        "                values = result_data[algorithm][prep][i]\n",
        "                for j, metric in enumerate(metrics):\n",
        "                    row[metric] = np.round(values[j], 3) if values[j] != \"NA\" else np.nan\n",
        "            except Exception:\n",
        "                for metric in metrics:\n",
        "                    row[metric] = np.nan\n",
        "            data.append(row)\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "methods_list = list(preprocessing_methods)\n",
        "df_kmeans = generate_results_table(\"KMeans\", methods_list, cluster_nums, results)\n",
        "df_agg = generate_results_table(\"Agglomerative\", methods_list, cluster_nums, results)\n",
        "df_meanshift = generate_results_table(\"MeanShift\", methods_list, cluster_nums, results)\n"
      ],
      "metadata": {
        "id": "WpJrKpF_yW6c"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_kmeans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUNY3bvv2swr",
        "outputId": "39ffe5ce-8856-4535-e527-0c335def8ded"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Preprocessing  Clusters  Silhouette  Calinski-Harabasz  \\\n",
            "0    No Data Processing         2       0.681            513.925   \n",
            "1    No Data Processing         4       0.498            530.766   \n",
            "2    No Data Processing         5       0.491            495.370   \n",
            "3   Using Normalization         2       0.630            354.366   \n",
            "4   Using Normalization         4       0.445            314.473   \n",
            "5   Using Normalization         5       0.353            289.506   \n",
            "6       Using Transform         2       0.582            251.349   \n",
            "7       Using Transform         4       0.387            207.266   \n",
            "8       Using Transform         5       0.346            202.952   \n",
            "9             Using PCA         2       0.706            570.839   \n",
            "10            Using PCA         4       0.558            719.124   \n",
            "11            Using PCA         5       0.552            685.027   \n",
            "12            Using T+N         2       0.582            251.349   \n",
            "13            Using T+N         4       0.387            207.266   \n",
            "14            Using T+N         5       0.346            202.952   \n",
            "15              T+N+PCA         2       0.615            283.005   \n",
            "16              T+N+PCA         4       0.441            264.488   \n",
            "17              T+N+PCA         5       0.416            278.549   \n",
            "\n",
            "    Davies-Bouldin  \n",
            "0            0.404  \n",
            "1            0.780  \n",
            "2            0.816  \n",
            "3            0.486  \n",
            "4            0.900  \n",
            "5            0.957  \n",
            "6            0.593  \n",
            "7            0.870  \n",
            "8            0.948  \n",
            "9            0.371  \n",
            "10           0.615  \n",
            "11           0.632  \n",
            "12           0.593  \n",
            "13           0.870  \n",
            "14           0.948  \n",
            "15           0.544  \n",
            "16           0.755  \n",
            "17           0.771  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_agg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2muuHF572stM",
        "outputId": "f2ef5cdb-60ff-4585-873d-04a5fe5933eb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Preprocessing  Clusters  Silhouette  Calinski-Harabasz  \\\n",
            "0    No Data Processing         2       0.687            502.822   \n",
            "1    No Data Processing         4       0.489            515.079   \n",
            "2    No Data Processing         5       0.484            488.485   \n",
            "3   Using Normalization         2       0.630            354.366   \n",
            "4   Using Normalization         4       0.433            301.104   \n",
            "5   Using Normalization         5       0.349            272.024   \n",
            "6       Using Transform         2       0.577            240.246   \n",
            "7       Using Transform         4       0.401            201.251   \n",
            "8       Using Transform         5       0.331            192.681   \n",
            "9             Using PCA         2       0.711            557.461   \n",
            "10            Using PCA         4       0.541            673.946   \n",
            "11            Using PCA         5       0.549            665.883   \n",
            "12            Using T+N         2       0.577            240.246   \n",
            "13            Using T+N         4       0.401            201.251   \n",
            "14            Using T+N         5       0.331            192.681   \n",
            "15              T+N+PCA         2       0.615            283.005   \n",
            "16              T+N+PCA         4       0.449            254.090   \n",
            "17              T+N+PCA         5       0.404            254.996   \n",
            "\n",
            "    Davies-Bouldin  \n",
            "0            0.383  \n",
            "1            0.795  \n",
            "2            0.820  \n",
            "3            0.486  \n",
            "4            0.849  \n",
            "5            0.906  \n",
            "6            0.592  \n",
            "7            0.979  \n",
            "8            0.974  \n",
            "9            0.349  \n",
            "10           0.655  \n",
            "11           0.653  \n",
            "12           0.592  \n",
            "13           0.979  \n",
            "14           0.974  \n",
            "15           0.544  \n",
            "16           0.723  \n",
            "17           0.791  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_meanshift)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlIogmZayW8z",
        "outputId": "6a042831-dfba-4ce9-cb07-d32c6e4fc592"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Preprocessing  Clusters  Silhouette  Calinski-Harabasz  \\\n",
            "0    No Data Processing         2         NaN                NaN   \n",
            "1    No Data Processing         4         NaN                NaN   \n",
            "2    No Data Processing         5         NaN                NaN   \n",
            "3   Using Normalization         2         NaN                NaN   \n",
            "4   Using Normalization         4         NaN                NaN   \n",
            "5   Using Normalization         5         NaN                NaN   \n",
            "6       Using Transform         2         NaN                NaN   \n",
            "7       Using Transform         4         NaN                NaN   \n",
            "8       Using Transform         5         NaN                NaN   \n",
            "9             Using PCA         2         NaN                NaN   \n",
            "10            Using PCA         4         NaN                NaN   \n",
            "11            Using PCA         5         NaN                NaN   \n",
            "12            Using T+N         2         NaN                NaN   \n",
            "13            Using T+N         4         NaN                NaN   \n",
            "14            Using T+N         5         NaN                NaN   \n",
            "15              T+N+PCA         2         NaN                NaN   \n",
            "16              T+N+PCA         4         NaN                NaN   \n",
            "17              T+N+PCA         5         NaN                NaN   \n",
            "\n",
            "    Davies-Bouldin  \n",
            "0              NaN  \n",
            "1              NaN  \n",
            "2              NaN  \n",
            "3              NaN  \n",
            "4              NaN  \n",
            "5              NaN  \n",
            "6              NaN  \n",
            "7              NaN  \n",
            "8              NaN  \n",
            "9              NaN  \n",
            "10             NaN  \n",
            "11             NaN  \n",
            "12             NaN  \n",
            "13             NaN  \n",
            "14             NaN  \n",
            "15             NaN  \n",
            "16             NaN  \n",
            "17             NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r9GDODr5Hn4S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}